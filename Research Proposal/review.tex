\section{Literature Review}
This section summarizes the overview of the two main topics of this research. The first part is the general introduction to active learning and current work on this topic, followed by the review on bayesioan nonparametric models sharing the same structure. 
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}
\newcommand{\argmin}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{min}}\;}
\subsection{Background}
Large Data sets are often heterogeneous, cased b amalgams from underlying sub-populations. When doing analysis on such data sets, it often involves grouping the data to make the original data more heterogeneous. Traditionally, parametric models using a fixed and finite number of parameters could be used, but they suffer from over- or under-fitting of data when there is a misfit between the complexity of the model and the amount of data available. Thus, model complexity selection is often an important issue in parametric modeling. But whether we use cross validation or marginal probabilities as the basis for selection, model selection is an operation that is fraught with difficulties, especially when the data set grows larger and larger. 

The Bayesian Nonparametric approach is an alternative to parametric modeling and selection. Under this scheme, the model comes with an unbounded complexity, and under-fitting and over-fitting are mitigated. Typically, we assume that the observed data set $x_1,x_2,\cdots,x_i$ are i.i.d(Independent Identical Distributed) sampled from some underlying unknown distribution $\mathbb{F}$. In Bayesian approach,  a prior is placed over $\mathbb{F}$ then he posterior over  $\mathbb{F}$ given data is computed. This prior over distributions is given by a parametric family. But constraining distributions to lie within parametric families limits the scope and type of inferences that can be made. Instead, the nonparametric approach used a prior over distributions with wide support, typically the support being the space of all distributions.  The Dirichlet Process is currently one of the most popular Bayesian Nonparametric Models. It was first formalized in [1] for general Bayesian statistical modeling.,as a prior over distributions with wide support yet tractable posteriors. But the Dirichlet Process is limited by the fact that draws from it are often discrete distributions, and tractable posterior is hard to obtian when it generalized to more general non-conjugate priors until MCMC techniques became available in the area. 
\subsection{Dirichlet Process}
\subsubsection{Dirichlet Distribution}
A Dirichlet distribution is defined on the $\mathit{(k-1)}$-dimensional probability simplex, whic is a surface in $\mathbb{R}^k$ denoted by $\Delta_k
$ and defined to be the set of vectors whose $k$ components are non-negative and sum to 1, that is
\begin{equation}
\Delta_k=\{q\in \mathbb{R}^k|\sum\nolimits_{i=1}^{k}q_i=1,q_i\geq 0\ for\ i=1,2,\cdots,k\}
\end{equation}
While the set $\Delta_k$ lies in a $k$-dimensional space, $\Delta_k$ is itself a $(k-1)$-dimensional object. Each point $q$ in the simplex can be thought of as a probability mass function(pmf) in its own right. The Dirichlet distribution can be thought of as a probability distribution over the $k-1$-dimensional probability simplex $\Delta_k$; that is, as a distribution over pmfs of length $k$.


\textbf{Dirichlet Distribution}: Let $Q=[Q_1,Q_2,\cdots,Q_k]$ be a random pmf, i.e. $Q_i\geq 0$ for $i=1,2,\cdots,k$ and $\sum\nolimits_{i=1}{k}Q_i=1$. In addition, suppose that $\alpha=[\alpha_1,\alpha_2,\cdots,\alpha_k]$ with $\alpha_i>0$ for each $i$, and let $\alpha_0=\sum\nolimits_{i=1^k\alpha_i}$. Then $Q$ is said to have a Dirichlet distribution with parameter $\alpha$, which we denote by $Q\sim Dir(\alpha)$, if it has $f(a;\alpha)=0$ then $q$ is not a pmf, else if $q$ is a pmf then
\begin{equation}
f(q;\alpha)=\frac{\Gamma(\alpha_0)}{\prod\nolimits_{i=1}^{k}\Gamma(\alpha_i)}\prod_{i=1}^{k}q_i^{\alpha_i-1}
\end{equation}
where $\Gamma(s)$ denotes the gamma function, a generalization of the factorial function, for $s>0, \Gamma(s+1)=s\Gamma(s)$, and for positive integers $n$, $\Gamma(n)=(n-1)!, \Gamma(1)=1$. 
\begin{figure}[tbph!]
	\centering
	\includegraphics[width=\linewidth]{DirDis}
	\caption{Dirichlet distribution density plots(blue=low, red=high) over the probability simplex in $\mathbb{R}^3$ for various values of the parameter $\alpha$}
	\label{fig:DirDis}
\end{figure}
Fig. \ref{fig:DirDis} shows the plots of the density fo the Dirichlet distribution over the two-dimensional probability simplex for $k=3$ events lying in three-dimensional Euclidean space over a variety of parameter vector $\alpha$. When $\alpha=[1,1,1]$, the Dirichlet distribution reduces to uniform distribution over the simplex. When all the components of $\alpha$ satisfy $\alpha_i>1$, the density is monomodal with its mode somewhere in the interior of the simplex. And when $\alpha_i<1$, the plots has sharp peaks at the vertices of the simplex. Another important feature should be mentioned, as can be seen from the definition of Dirichlet distribution, is that the support is open thus does not include the vertices or edge of the simplex, which in reality means that no component of a pmf drawn from a Dirichlet will ever be zero.  Fig. \ref{fig:DPsample} shows plots of samples drawn i.i.d from different Dirichlet distributions.

\begin{figure}[tbph!]
	\centering
	\includegraphics[width=\linewidth]{DPsample}
	\caption{Plots of sample pmfs drawn from Dirichlet distributions over the probability simplex in $\mathbb{R}^3$ for various values of parameter $\alpha$}
	\label{fig:DPsample}
\end{figure}
\subsubsection{Dirichlet Process}
A Dirichlet Process is a distribution over probability distributions. Suppose that $\mathit{G}$ is a probability distribution over a measurable space $\Theta$, then  $\mathit{G}$ is a probability distribution over $\Theta$ and a DP is a distribution over all such distributions. A DP is parameterized b a concentration parameter $\alpha$ and a base measure(base distribution) $\mathit{H}$. So, the formal definition of a DP would be
\begin{equation}
G\sim  DP(\alpha,\mathit{H})
\end{equation}
it means for a finite set of measurable paritions $A_1\cup\cdots\cup A_k=\Theta$, 
\begin{equation}
(G(A_1)\cdots,G(A_k))\sim Dir(\alpha H(A_1),\cdots,\alpha H(A_k))
\end{equation}
which means the probabilities that  $\mathit{G}$ assigns to any finite
partition of  $\Theta$ follow a Dirichlet distribution  with parameters
$(\alpha H(A_1),\cdots,\alpha H(A_k))$.

Since $G$ is a random distribution we can inturn draw samples from $G$ itself. Let $\theta_1,\theta_2,\cdots,\theta_n$ be a sequence of independent draws from $G$. What is the posterior distribution of $G$ given observed values of $\theta_1,\theta_2,\cdots,\theta_n$. In other words, what is the posterior predictive distribution for a new item $p(\theta_{N+1}|\theta_1,\theta_2,\cdots,\theta_n)=\int p(\theta_{N+1}|G)p(G|\theta_1,\theta_2,\cdots,\theta_n)dG$? It is shown in ... that 
\begin{equation}
	(G(A_1),\cdots,G(A_k))|(\theta_1,\theta_2,\cdots,\theta_n)\sim Dir((\alpha H(A_1)+n_1,\cdots,\alpha H(A_k)+n_k)
\end{equation}
where $n_k=\#\{i:\theta_i\in A_k\}$ is the number of observed values in $A_k$. Rewriting the posterior DP, we have:
\begin{equation}
G|(\theta_1,\theta_2,\cdots,\theta_n)\sim DP(\alpha+n,\frac{\alpha}{\alpha+n}H+\frac{n}{\alpha+n}\frac{\sum\nolimits_{i=1}^{n}\delta(\theta_i)}{n})
\end{equation}
and for the posterior distribution,
\begin{equation}
\begin{split}
p(\theta_{N+1}|\theta_1,\theta_2,\cdots,\theta_n)&= E[G(A)|\theta_1,\theta_2,\cdots,\theta_n] \\
&=\frac{1}{\alpha+n}\left(\alpha H+\sum\limits_{i=1}^{n}\delta_{\theta_i}\right)
\end{split}\label{eqa:dppos}
\end{equation}
Therefore the posterior base distribution given $\theta_1,\theta_2,\cdots,\theta_n$ is also the predictive distribution of $\theta_{n+1}$. The sequence of predictive distribution \ref{eqa:dppos} for $\theta_1,\theta_2,\cdots,\theta_n$ is very important and different interpretations on it indicate different properties of the result. To better understand it, we will introduce the most famous three representations of \ref{eqa:dppos}: \textbf{P\'{o}lya urn, Chinese Restaurant Process, Stick-breaking Process}.
\begin{itemize}
	\item \textbf{P\'{o}lya urn} \\
	In this analogy, suppose we are drawing colored balls from an urn, $\theta_i$ represents the color of the $i$-th ball drawn . For each ball drawn, we place it back  and add another one in the same color into the urn. In  the beginning, we pick a color drawn from $H$, paint a ball with that color and drop it into the urn. In the following $n$th step, we will either, pick a new color with probability$\frac{\alpha}{\alpha+n}$, or with probability $\frac{n}{\alpha+n}$ pick a random ball out of the urn.  This process induces a "rich get richer" property on the frequencies of colors inside the urn. Also, it should be noticed that the predictive distribution has point masses located at the previous draws  $\theta_1,\theta_2,\cdots,\theta_n$. Thus the distribution $G$ itself has point masses. When sample size grows larger, the value of any draw will be repeated by another draw, implying that $G$ is composed only of a weighted sum of point masses and thus  it is a discrete distribution.
	\item \textbf{Chinese Restaurant Process}\\
	Another representation of Dirichlet Process-Chinese Restaurant Process(CRP) implies a clustering property. Equation \ref{eqa:dppos} could be rewrite as :
	\begin{equation}
		p(\theta_{N+1}|\theta_1,\theta_2,\cdots,\theta_n) = \frac{1}{\alpha+n}\left(\alpha H+\sum\limits_{i=1}^{n}n_k\delta_{\theta_i^\star}\right)
	\end{equation}
	where $\theta_i^\star$ is the unique values among $\theta_1,\theta_2,\cdots,\theta_n$, and $n_k$ is the number of repeats of $\theta_i^\star$. $\theta_i^\star$ will be repeated by $\theta_{n+1}$ with probability proportional to $n_k$. The larger $n_k$ is, the hight the probability that it will grow. This is similar to the "rich get richer" scheme in \textbf{P\'{o}lya urn}, where large clusters grow larger. We can see that the unique values  of $\theta_1,\theta_2,\cdots,\theta_n$ induce a partitioning of the set $[n]=\{1,2,\cdots,n\}$ into clusters such that within some cluster $k$, the $\theta_i$'s take on the same value $\theta_k^\star$. The random partion encapsulates all the properties of the DP. If we invert the generative process, we can reconstruct the joint distribution over $\theta_1,\theta_2,\cdots,\theta_n$ by first drawin ga random partion on $[n]$, then each cluster $k$in the partition draw a $\theta_k^\star\sim H$, and finally assign $\theta_i=\theta_k^\star$ for each $i$ in cluster $k$. 
	
	The distribution over partitions is called the Chinese Restaurant Process(CRP) in which we have a Chinese restaurant with infinite tables, each of which can seat an infinite number of customers. The first customer enters the restaurant and sits at the first table. And in the following, when the $n+1$th customer comes, he will either joins a tale $k$ with probability propotional to the number $n_k$ of people already sitting there or sts at a new table with probability propotional to $\alpha$. The CRP define a distribution over partitions of $[n]$ and a distribution over permutations of $[n]$. 
	\item \textbf{Stick-breaking Construction} 
	The  third representation of DP is very intuitive and the most widely used one to generate a sample from it. By knowing the fact that draws from a DP consists actually of a weighted sum of point masses, ...provides a constructive and straightforward definition of the DP. It simply following the flowwin steps.
\begin{eqnarray}
	\beta_k\sim Beta(1,\alpha)&\quad&\theta_k^\star\sim H,\\
	\pi_k=\beta_k\prod_{l=1}^{k-1}(1-\beta_k)&\quad&G=\sum_{k=1}^{\infty}\pi_k\delta_k^\star,\\
\end{eqnarray}
Then we have $G\sim DP(\alpha,H)$. The construction of $\pi$ could be interpreted as breaking a stick of length 1 step by step. First break it at $\beta_1$, assigning $\pi_1$ to be the length of segment we just got. Then recursively breaking the remaining portion to obtain $\pi_2,\pi_3,\cdots,\pi_{n-1}$. THis is a very straightforward and simple procedure.
\end{itemize}   

\subsubsection{ Dirichlet Process Mixture Models}\label{bnp}
In this section, we will briefly describe the Dirichlet Process Mixture Model
Most machine learning problem is targeted to learn a set of parameters describing the model from training data set. This sort of learning is often evaluated in two terms, with the first one being how well the model fits the data, expressed as accuracy or squared error, and the second one being a complexity penalty(favoring simpler models)\cite{gershman2012tutorial,escobar1995bayesian}, also referred to as Ocam's Razor. In practical problem, the model complexity is hard to evaluate. Improper model complexity will lead to over-fitting or under-fitting\cite{gershman2012tutorial,muller2004nonparametric}, which will then affect the model generalization, i.e to be applied to practical use. Although through rigid training, desirable models could be trained but this is basically a trial and error process\cite{hjort2010bayesian}. That is the movivation of discovering adaptive model complexity selection methods, among which Bayesian Nonparametric Method is the most widely used.

DP, as the most widely used methods among Bayesian Nonparametric Models, has found applications in  both statistics and machine learning, including Bayesian model validation, density estimation and clustering via  mixture models, among which the last one is the most salient  when talking about DP. 

Model validation is to evaluate whether a model gives a good fit to observed data. Under Bayesian approach, we would usually compute the marginal probability of the data under  the model  and compare the  marginal probability to that of other candidate model. The one with the highest probability will be chosen as the best fitting to the observed data. Here arises an issue that how to choose the models to be compared. Usually, a set of candidate models as large as possible would be desirable. But it would be easier if we re-think this in a Bayesian Nonparametric way, i.e. to use the space of all possible distribution as our comparison class, with a prior over distributions. The DP is always the first priority for its similar nature to this problem. The approach is to use the given parametric model as the base distribution of the DP, with DP serving as a nonparametric relaxation around this parametric model. If the parametric model performs as well or better than the DP relaxed model, we are convinced that the model is valid.

For density estimation, the aim is to modeling the latent density from which the observed data is drawn. To avoid the poor performance caused by the limitation in parametric model, we again employ a Nonparametric prior over all densities. If we drawn samples from a DP, which is  distribution over distribution, we will obtain a random distribution which is discrete,thus has no densities. The solution is to smooth out draws form the DP with a kernel. Let $G\sim DP(\alpha,H)$ and $f(x|\theta)$ be a family of densities indexed by $\theta$. Then 
\begin{equation}
p(x)=\int f(x|\theta)G(\theta)d\theta
\end{equation}

The most common application of the Dirichlet process is to cluster data using mixture  models. The traditional finite mixture model assumes that there are $K$ clusters, each associated with a parameter $\theta_k$. Each observation $y_n$ is assumed to be generated by first  choosing a sluster $c_n$ according to $P(c_n)$ and then generating the observation from its corresponding observation destribution parameterized by $\theta_{c_n}$. Finite model can accomodate many kinds of data by changing the data generating distribution.

Bayesian mixture models further contain a prior over the mixing distributions. The nature of Dirichlet process will translate the mixing model to a countably infinite number of components. we model a set of observations $\{x_1,\cdots,x_n\}$ using a set of latent parameters  $\{\theta_1,\cdots,\theta_n\}$, each $\theta_i$ is drawn I.I.d from $G$, while each $x_i$ has distribution $F(\theta_i)$ parameterized by $\theta_i$:
\begin{eqnarray}
x_i|\theta_i&\sim& F(\theta_i)\\
\theta_i|G&\sim& G\\
G|\alpha,H&\sim& DP(\alpha,H)
\end{eqnarray}\label{eqa:DPM}
Because $G$ is discrete, multiple $\theta_i$'s can take on the same value simultaneously, and the model above can be seen as a mixture model, where $x_i$'s with the same value of $\theta_i$ belong to the same cluster. The mixture perspective can be made more in agreement with the usual representation of mixture models using the stick-breaking construction. Let $z_i$ be a cluster assignment variable, which takes on value $k$ with probability $\pi_k$. Then equation \ref{eqa:DPM} could be expressed as:
\begin{eqnarray}
\pi|\alpha\sim GEM(\alpha)& \quad &\theta_k^\star|H\sim H \\
z_i|\pi\sim  Mult(\pi)& \quad & x_i|z_i,\{\theta_k^\star\} \sim F(\theta_{z_i}^\star)
\end{eqnarray}
with $G=\sum\nolimits_{k=1}^\infty\pi_k\delta_{\theta_k^\star}$,  $\theta_i=\theta_{z_i}^\star$, $\pi$ being the mixing proportion, $\theta_k^\star$ being the cluster parameters, $F(\theta_k^\star)$ being the distribution over data in cluster $k$ and $H$ the prior over cluster parameters.

From above expression, it is seen that DP mixture model is an infinite mixture model-a mixture model with a countably infinite number of clusters. Different from finite mixture model using a fixed number of clusters, $\pi_k$'s decrease exponentially quickly, and only a small number of clusters will be used to model the data a priori. In the DP mixture model, the actual number of clusters used to model data is not fixed, and can be automatically inferred from data using the usual Bayesian posterior inference framework. The equivalent operation for finite mixture models would be model averaging or model selection for the appropriate number of components.
\begin{tabular}{|c|c|c|}
	\hline \rule[-2ex]{0pt}{5.5ex}  & \textbf{Indexing}& \textbf{Partition Labels} \\ 
	\hline \rule[-2ex]{0pt}{5.5ex}  \textbf{P\'{o}lya Urn & sequence of draws of balls  & ball colors  \\ 
	\hline \rule[-2ex]{0pt}{5.5ex} \textbf{Chinese Restaurant Process} & sequence of incoming customers &  different dishes\\ 
	\hline \rule[-2ex]{0pt}{5.5ex}  \textbf{Clustering}&sequence of the natural numbers  & clusters \\ 
	\hline 
\end{tabular} 
\subsection{Inference}



\subsection{Summary}

In this section, we briefly review the two main topic in this proposed research. Generally, Active Learning is an important subsection in machine learning and very promising since it aims to address the bottleneck due to insufficient training instances. We exploited the general problem and main aspects in active learning. And for Bayesian Nonparametric models, it tries to overcome the shortcomings in traditional parametric methods and the flexibility allows for adaption of the model to fit the data,  this is especially important on the model generalization. 

By the analysis in this section, these two branches could be complementary to each other if these are used under the same famework and form a loop of feedback for each other's training stage. BNP model will cluster the dataset adaptively and recommend the most representative sample for active learning. Similarly, active learning will in returen recommend the BNP model which set of point is more preferred which will guide the BNP model updating. 

This close-loop framework is the one we are desired to develop, the performance of similar frameworks in the other area has been proved\cite{zhang2011close}.